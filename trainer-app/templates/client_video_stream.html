<html>
    <head>

    </head>
    <body>
        <video id="video" width="480" height="320" autoplay playsinline></video>
        <canvas id="canvas"></canvas>


        













        <!-- Require the peer dependencies of pose-detection. -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>

<!-- You must explicitly require a TF.js backend if you're not using the TF.js union bundle. -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
<!-- Alternatively you can use the WASM backend: <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm/dist/tf-backend-wasm.js"></script> -->

<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection"></script>

<script>
    const videoElement = document.getElementById('video');

navigator.mediaDevices.getUserMedia({ video: true, audio: false })
    .then(stream => {
      videoElement.srcObject = stream;
      videoElement.play();
    })
    .catch(err => {
      alert(`Following error occured: ${err}`);
    });
    const canvas = document.getElementById('canvas');

async function loadAndBlur() {
    const detectorConfig = {
  modelType: poseDetection.movenet.modelType.SINGLEPOSE_LIGHTNING,
  enableTracking: true,
  trackerType: poseDetection.TrackerType.BoundingBox
};

const detector = await poseDetection.createDetector(poseDetection.SupportedModels.MoveNet, detectorConfig);
const poses = await detector.estimatePoses(videoElement);
  

  const backgroundBlurAmount = 9;
  const edgeBlurAmount = 3;
  const flipHorizontal = false;

  bodyPix.drawBokehEffect(canvas, videoElement, poses);
}
loadAndBlur();
</script>
    </body>
</html>