<html>
    <head>

    </head>
    <body>
        <video id="video" width="480" height="320" autoplay playsinline></video>
        <canvas id="canvas">CANVAS</canvas>


        













        <!-- Require the peer dependencies of pose-detection. -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>

<!-- You must explicitly require a TF.js backend if you're not using the TF.js union bundle. -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
<!-- Alternatively you can use the WASM backend: <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm/dist/tf-backend-wasm.js"></script> -->

<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection"></script>

 <!-- Import TensorFlow.js -->
 <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.0.0/dist/tf.min.js"></script>
 <!-- Import tfjs-vis -->
 <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-vis@1.0.2/dist/tfjs-vis.umd.min.js"></script>


 <script src="{{url_for('static',filename='js/webstream.js')}}"></script>

<script>
    const videoElement = document.getElementById('video');

navigator.mediaDevices.getUserMedia({ video: true, audio: false })
    .then(stream => {
      videoElement.srcObject = stream;
      videoElement.play();
    })
    .catch(err => {
      alert(`Following error occured: ${err}`);
    });
    const canvas = document.getElementById('canvas');

async function loadAndBlur() {
    const detectorConfig = {
  modelType: poseDetection.movenet.modelType.SINGLEPOSE_LIGHTNING,
  enableTracking: true,
  trackerType: poseDetection.TrackerType.BoundingBox
};

const detector = await poseDetection.createDetector(poseDetection.SupportedModels.MoveNet, detectorConfig);
const poses = await detector.estimatePoses(videoElement);
console.log(poses[0].keypoints);
}
loadAndBlur();
</script>
    </body>
</html>